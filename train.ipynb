{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  f1_score\n",
    "from utils import load_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к файлу с мета данными\n",
    "meta_pth = '/media/grigory/Диск/ITMO_DATA/data_v_7_stc/meta/meta.txt'\n",
    "\n",
    "# Пути к директориям с аудиофайлами.\n",
    "train_audio_pth = '/media/grigory/Диск/ITMO_DATA/data_v_7_stc/audio'\n",
    "test_audio_pth = '/media/grigory/Диск/ITMO_DATA/data_v_7_stc/test'\n",
    "\n",
    "extracted_data = 'data/extracted'\n",
    "\n",
    "# Имена файлов с извлечёнными признаками для dense сети, линейных моделей и деревьев.\n",
    "extracted_train = 'features_labels_train' \n",
    "extracted_test = 'features_labels_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загружаем извлечённые признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, names, labels = load_obj(os.path.join(extracted_data, extracted_train))\n",
    "X_test, names_test = load_obj(os.path.join(extracted_data, extracted_test))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# кодируем таргет\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "ohe =  OneHotEncoder()\n",
    "y_ohe = ohe.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число классов: 8\n",
      "Число признаков: 193\n",
      "Число сэмплов: 11307\n"
     ]
    }
   ],
   "source": [
    "n_features = X.shape[1]\n",
    "n_classes = y.max()+1\n",
    "print('Число классов: {}\\nЧисло признаков: {}\\nЧисло сэмплов: {}'.format(n_classes, n_features, y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> для правильной валидации нужно учитывать, что некоторые сэмплы являются частями одной записи.\n",
    "Если части одной записи будут разрознено находится и в train и в val, то произойдёт утечка меток в валидацию.\n",
    "\n",
    "Поэтому сперва найдём уникальные записи и сгруппируем все фрагменты по принадлежности к отдельной записи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_le = LabelEncoder()\n",
    "unique_samples = pd.Series(names).apply(lambda x: x.split('time_stretch')[0].strip('.wav').strip('_'))\n",
    "groups = groups_le.fit_transform(unique_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8, random_state=7)\n",
    "\n",
    "idxs = np.arange(X.shape[0])\n",
    "tr_idxs, val_idxs = next(iter(gss.split(idxs, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сплитим...\n",
    "# имена файлов\n",
    "names_train, names_val = names[tr_idxs], names[val_idxs]\n",
    "# извлечённые признаки\n",
    "X_train, X_val = X[tr_idxs], X[val_idxs]\n",
    "# метки для sklearn\n",
    "y_train, y_val = y[tr_idxs], y[val_idxs]\n",
    "# метки для сеток\n",
    "y_ohe_train, y_ohe_val = y_ohe[tr_idxs], y_ohe[val_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update:**\n",
    "ниже некорректный сплит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # сплитим\n",
    "# tmp = train_test_split(names, X, y, y_ohe, train_size=0.8, test_size=0.2, stratify=y)\n",
    "\n",
    "# # имена файлов, входные данные, метки для sklearn, метки для сеток\n",
    "# names_train, names_val, X_train, X_val, y_train, y_val, y_ohe_train, y_ohe_val = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# C_grid = np.logspace(-3,4,10) \n",
    "# lr = LogisticRegressionCV(C_grid, n_jobs=-1) #LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "CPU times: user 10.1 s, sys: 572 ms, total: 10.7 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Обучаем\n",
    "# print('Training...')\n",
    "# lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...\n",
      "Acc:: Train score: 0.91414029652578, test score: 0.879682679594535\n",
      "F1:: Train score: 0.913935660013014, test score: 0.8805494911071801\n"
     ]
    }
   ],
   "source": [
    "# # Оцениваем качества на отложенной выборке\n",
    "# print('Scoring...')\n",
    "# tr_score = lr.score(X_train, y_train)\n",
    "# te_score = lr.score(X_val, y_val)\n",
    "# print('Acc:: Train score: {}, test score: {}'.format(tr_score, te_score))\n",
    "\n",
    "# tr_score = f1_score(y_train, lr.predict(X_train), average='weighted')\n",
    "# te_score = f1_score(y_val, lr.predict(X_val), average='weighted')\n",
    "# print('F1:: Train score: {}, test score: {}'.format(tr_score, te_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нелинейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### бустинг и лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# # !pip install lightgbm\n",
    "# import lightgbm as lgb\n",
    "# from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "# trees = LGBMClassifier(learning_rate=1e-2,\n",
    "# #                        reg_alpha=1e-2,\n",
    "# #                        reg_beta=1e-1,\n",
    "# #                      valid_sets=[X_val, y_val],\n",
    "#                        random_state=7,\n",
    "#                      n_estimators=500)\n",
    "# trees.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# trees = RandomForestClassifier(n_estimators=100, max_depth=25, max_features=120)\n",
    "\n",
    "# trees.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...\n",
      "Acc:: Train score: 1.0, test score: 0.9471132657558395\n"
     ]
    }
   ],
   "source": [
    "# # Оцениваем качества на отложенной выборке\n",
    "# print('Scoring...')\n",
    "# tr_score = trees.score(X_train, y_train)\n",
    "# te_score = trees.score(X_val, y_val)\n",
    "# print('Acc:: Train score: {}, test score: {}'.format(tr_score, te_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "переобучились немножк..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### онлайн-обучение нейонных сеток\n",
    "\n",
    "Обзор методов на нейронных сетях:\n",
    "\n",
    "- http://www.fim.uni-passau.de/fileadmin/files/lehrstuhl/schuller/Publications/Amiriparian17-SSC.pdf\n",
    "- https://github.com/libphy/which_animal\n",
    "- https://github.com/jaron/deep-listening\n",
    "- https://musicinformationretrieval.com/mfcc.html\n",
    "\n",
    "Тестировались:\n",
    "    - полносвязанные сети\n",
    "    - lstm\n",
    "    - свёрточные\n",
    "\n",
    "Самые лучшие результаты показали полносвязные сети.\n",
    "\n",
    "\n",
    "*Коротко о подборе оптимальной архитектуры для Dense сети*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=350 src=\"https://habrastorage.org/webt/sg/7t/tu/sg7ttuirleaml3_j7dwo2tn0iqs.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import (build_deep_dense, build_conv_seq, build_lstm_seq)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: val_loss improved from inf to 1.01507, saving model to models_weights/net_ff.h5\n",
      "Epoch 00008: val_loss improved from 1.01507 to 0.64368, saving model to models_weights/net_ff.h5\n",
      "Epoch 00012: val_loss improved from 0.64368 to 0.48766, saving model to models_weights/net_ff.h5\n",
      "Epoch 00016: val_loss improved from 0.48766 to 0.41567, saving model to models_weights/net_ff.h5\n",
      "Epoch 00020: val_loss improved from 0.41567 to 0.35174, saving model to models_weights/net_ff.h5\n",
      "Epoch 00024: val_loss improved from 0.35174 to 0.33874, saving model to models_weights/net_ff.h5\n",
      "Epoch 00028: val_loss improved from 0.33874 to 0.30450, saving model to models_weights/net_ff.h5\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 00036: val_loss improved from 0.30450 to 0.25036, saving model to models_weights/net_ff.h5\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 00048: val_loss improved from 0.25036 to 0.22810, saving model to models_weights/net_ff.h5\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 00056: val_loss improved from 0.22810 to 0.22158, saving model to models_weights/net_ff.h5\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 00068: val_loss improved from 0.22158 to 0.19701, saving model to models_weights/net_ff.h5\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 00104: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda7df9a748>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_deep_dense(n_features, n_classes)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(1e-4), \n",
    "              metrics=['accuracy'])\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=25, monitor='val_loss'),\n",
    "    ModelCheckpoint('models_weights/net_ff.h5', monitor='val_loss', \n",
    "                    verbose=1, save_best_only=True, period=4)\n",
    "]\n",
    "model.fit(X_train, y_ohe_train.todense(), \n",
    "          epochs=300, batch_size=250, \n",
    "          verbose=0,\n",
    "          validation_data=[X_val, y_ohe_val.todense()],\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Рекурретная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 92491 samples, validate on 21060 samples\n",
      "Epoch 1/300\n",
      "92491/92491 [==============================] - 51s 549us/step - loss: 0.8721 - acc: 0.7172 - val_loss: 0.4160 - val_acc: 0.8614\n",
      "Epoch 2/300\n",
      "92491/92491 [==============================] - 49s 528us/step - loss: 0.3717 - acc: 0.8784 - val_loss: 0.3155 - val_acc: 0.8955\n",
      "Epoch 3/300\n",
      "92491/92491 [==============================] - 50s 536us/step - loss: 0.3038 - acc: 0.9006 - val_loss: 0.2986 - val_acc: 0.9009\n",
      "Epoch 4/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.9143Epoch 00004: val_loss improved from inf to 0.27935, saving model to models_weights/net_lstm.h5\n",
      "92491/92491 [==============================] - 49s 531us/step - loss: 0.2630 - acc: 0.9143 - val_loss: 0.2793 - val_acc: 0.9088\n",
      "Epoch 5/300\n",
      "92491/92491 [==============================] - 49s 530us/step - loss: 0.2353 - acc: 0.9231 - val_loss: 0.2605 - val_acc: 0.9162\n",
      "Epoch 6/300\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.2162 - acc: 0.9296 - val_loss: 0.2564 - val_acc: 0.9183\n",
      "Epoch 7/300\n",
      "92491/92491 [==============================] - 49s 530us/step - loss: 0.2020 - acc: 0.9331 - val_loss: 0.2463 - val_acc: 0.9220\n",
      "Epoch 8/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9388Epoch 00008: val_loss improved from 0.27935 to 0.25754, saving model to models_weights/net_lstm.h5\n",
      "92491/92491 [==============================] - 49s 530us/step - loss: 0.1880 - acc: 0.9387 - val_loss: 0.2575 - val_acc: 0.9165\n",
      "Epoch 9/300\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1773 - acc: 0.9421 - val_loss: 0.2271 - val_acc: 0.9258\n",
      "Epoch 10/300\n",
      "92491/92491 [==============================] - 49s 530us/step - loss: 0.1668 - acc: 0.9457 - val_loss: 0.2494 - val_acc: 0.9189\n",
      "Epoch 11/300\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1591 - acc: 0.9474 - val_loss: 0.2311 - val_acc: 0.9294\n",
      "Epoch 12/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9504Epoch 00012: val_loss improved from 0.25754 to 0.23769, saving model to models_weights/net_lstm.h5\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1513 - acc: 0.9503 - val_loss: 0.2377 - val_acc: 0.9260\n",
      "Epoch 13/300\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1437 - acc: 0.9530 - val_loss: 0.2257 - val_acc: 0.9318\n",
      "Epoch 14/300\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1368 - acc: 0.9558 - val_loss: 0.2206 - val_acc: 0.9323\n",
      "Epoch 15/300\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1332 - acc: 0.9563 - val_loss: 0.2243 - val_acc: 0.9329\n",
      "Epoch 16/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9588Epoch 00016: val_loss improved from 0.23769 to 0.21225, saving model to models_weights/net_lstm.h5\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1269 - acc: 0.9588 - val_loss: 0.2122 - val_acc: 0.9369\n",
      "Epoch 17/300\n",
      "92491/92491 [==============================] - 49s 530us/step - loss: 0.1210 - acc: 0.9598 - val_loss: 0.2174 - val_acc: 0.9390\n",
      "Epoch 18/300\n",
      "92491/92491 [==============================] - 49s 530us/step - loss: 0.1175 - acc: 0.9621 - val_loss: 0.2371 - val_acc: 0.9297\n",
      "Epoch 19/300\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1128 - acc: 0.9633 - val_loss: 0.2261 - val_acc: 0.9349\n",
      "Epoch 20/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9655Epoch 00020: val_loss did not improve\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.1084 - acc: 0.9655 - val_loss: 0.2286 - val_acc: 0.9337\n",
      "Epoch 21/300\n",
      "92491/92491 [==============================] - 49s 531us/step - loss: 0.1040 - acc: 0.9668 - val_loss: 0.1995 - val_acc: 0.9417\n",
      "Epoch 22/300\n",
      "92491/92491 [==============================] - 49s 532us/step - loss: 0.1010 - acc: 0.9673 - val_loss: 0.2349 - val_acc: 0.9345\n",
      "Epoch 23/300\n",
      "92491/92491 [==============================] - 49s 531us/step - loss: 0.0988 - acc: 0.9677 - val_loss: 0.2166 - val_acc: 0.9375\n",
      "Epoch 24/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9688Epoch 00024: val_loss did not improve\n",
      "92491/92491 [==============================] - 49s 531us/step - loss: 0.0949 - acc: 0.9687 - val_loss: 0.2295 - val_acc: 0.9396\n",
      "Epoch 25/300\n",
      "92491/92491 [==============================] - 49s 531us/step - loss: 0.0930 - acc: 0.9692 - val_loss: 0.2240 - val_acc: 0.9369\n",
      "Epoch 26/300\n",
      "92491/92491 [==============================] - 49s 531us/step - loss: 0.0900 - acc: 0.9711 - val_loss: 0.2183 - val_acc: 0.9398\n",
      "Epoch 27/300\n",
      "92491/92491 [==============================] - 49s 531us/step - loss: 0.0879 - acc: 0.9713 - val_loss: 0.2408 - val_acc: 0.9371\n",
      "Epoch 28/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9731Epoch 00028: val_loss did not improve\n",
      "92491/92491 [==============================] - 49s 535us/step - loss: 0.0838 - acc: 0.9731 - val_loss: 0.2363 - val_acc: 0.9387\n",
      "Epoch 29/300\n",
      "92491/92491 [==============================] - 50s 537us/step - loss: 0.0805 - acc: 0.9737 - val_loss: 0.2424 - val_acc: 0.9344\n",
      "Epoch 30/300\n",
      "92491/92491 [==============================] - 50s 543us/step - loss: 0.0805 - acc: 0.9737 - val_loss: 0.2365 - val_acc: 0.9370\n",
      "Epoch 31/300\n",
      "92491/92491 [==============================] - 50s 543us/step - loss: 0.0778 - acc: 0.9743 - val_loss: 0.2259 - val_acc: 0.9396\n",
      "Epoch 32/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9755Epoch 00032: val_loss did not improve\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.0757 - acc: 0.9755 - val_loss: 0.2384 - val_acc: 0.9393\n",
      "Epoch 33/300\n",
      "92491/92491 [==============================] - 49s 526us/step - loss: 0.0740 - acc: 0.9761 - val_loss: 0.2305 - val_acc: 0.9389\n",
      "Epoch 34/300\n",
      "92491/92491 [==============================] - 49s 527us/step - loss: 0.0737 - acc: 0.9757 - val_loss: 0.2401 - val_acc: 0.9380\n",
      "Epoch 35/300\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.0697 - acc: 0.9775 - val_loss: 0.2506 - val_acc: 0.9354\n",
      "Epoch 36/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9776Epoch 00036: val_loss did not improve\n",
      "92491/92491 [==============================] - 49s 529us/step - loss: 0.0701 - acc: 0.9775 - val_loss: 0.2200 - val_acc: 0.9410\n",
      "Epoch 37/300\n",
      "92491/92491 [==============================] - 49s 528us/step - loss: 0.0678 - acc: 0.9779 - val_loss: 0.2292 - val_acc: 0.9401\n",
      "Epoch 38/300\n",
      "92491/92491 [==============================] - 49s 526us/step - loss: 0.0655 - acc: 0.9789 - val_loss: 0.2495 - val_acc: 0.9341\n",
      "Epoch 39/300\n",
      "92491/92491 [==============================] - 49s 527us/step - loss: 0.0661 - acc: 0.9781 - val_loss: 0.2307 - val_acc: 0.9422\n",
      "Epoch 40/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9791Epoch 00040: val_loss did not improve\n",
      "92491/92491 [==============================] - 49s 528us/step - loss: 0.0632 - acc: 0.9792 - val_loss: 0.2205 - val_acc: 0.9433\n",
      "Epoch 41/300\n",
      "92491/92491 [==============================] - 49s 526us/step - loss: 0.0598 - acc: 0.9804 - val_loss: 0.2430 - val_acc: 0.9389\n",
      "Epoch 42/300\n",
      "92491/92491 [==============================] - 49s 525us/step - loss: 0.0591 - acc: 0.9809 - val_loss: 0.2548 - val_acc: 0.9380\n",
      "Epoch 43/300\n",
      "92491/92491 [==============================] - 49s 527us/step - loss: 0.0590 - acc: 0.9810 - val_loss: 0.2524 - val_acc: 0.9408\n",
      "Epoch 44/300\n",
      "92250/92491 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9807Epoch 00044: val_loss did not improve\n",
      "92491/92491 [==============================] - 49s 525us/step - loss: 0.0581 - acc: 0.9808 - val_loss: 0.2423 - val_acc: 0.9386\n",
      "Epoch 45/300\n",
      "92491/92491 [==============================] - 48s 524us/step - loss: 0.0573 - acc: 0.9809 - val_loss: 0.2715 - val_acc: 0.9403\n",
      "Epoch 46/300\n",
      "92491/92491 [==============================] - 49s 526us/step - loss: 0.0554 - acc: 0.9818 - val_loss: 0.2507 - val_acc: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdab0ba5f98>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = build_lstm_seq(timesteps=20, data_dim=41, n_classes=n_classes)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=Adam(1e-4), \n",
    "#               metrics=['accuracy'])\n",
    "# callbacks = [\n",
    "#     EarlyStopping(patience=25, monitor='val_loss'),\n",
    "#     ModelCheckpoint('models_weights/net_lstm.h5', monitor='val_loss', \n",
    "#                     verbose=1, save_best_only=True, period=4)\n",
    "# ]\n",
    "\n",
    "# model.fit(X_train, y_ohe_train.todense(), \n",
    "#           epochs=300, batch_size=250, \n",
    "#           validation_data=[X_val, y_ohe_val.todense()],\n",
    "#          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "model = load_model('models_weights/net_ff.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:: Train score: 0.9848220389975106, val score: 0.9530651061049963\n"
     ]
    }
   ],
   "source": [
    "from utils import inverse_ohe\n",
    "preds = model.predict(X_train)\n",
    "preds_labels = inverse_ohe(preds, ohe)\n",
    "tr_score = f1_score(y_train, preds_labels, average='weighted')\n",
    "preds = model.predict(X_val)\n",
    "preds_labels = inverse_ohe(preds, ohe)\n",
    "te_score = f1_score(y_val, preds_labels, average='weighted')\n",
    "print('F1:: Train score: {}, val score: {}'.format(tr_score, te_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы по моделям:\n",
    "\n",
    "> Наилучшими моделями(без яростного тюнинга параметров) оказались полносвязная сеть и бустинг над решающими деревьями. Они дают  сопоставимые результаты.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предикт теста и подготовка сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submit(names, preds, ohe=ohe, le=le):\n",
    "    pred_labels = inverse_ohe(preds, ohe)\n",
    "    pred_names = le.inverse_transform(pred_labels)\n",
    "    df = pd.DataFrame(list(zip(names, preds.max(axis=1), pred_names)), columns=['file','prob','label'])\n",
    "    return df\n",
    "                      \n",
    "submit_df = prepare_submit(names_test, preds)\n",
    "submit_df.to_csv('result.txt', sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
