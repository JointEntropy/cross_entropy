{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  f1_score, accuracy_score\n",
    "from utils import load_obj\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к файлу с мета данными\n",
    "meta_pth = '/media/grigory/Диск/ITMO_DATA/data_v_7_stc/meta/meta.txt'\n",
    "\n",
    "# Пути к директориям с аудиофайлами.\n",
    "train_audio_pth = '/media/grigory/Диск/ITMO_DATA/data_v_7_stc/audio'\n",
    "test_audio_pth = '/media/grigory/Диск/ITMO_DATA/data_v_7_stc/test'\n",
    "\n",
    "extracted_data = 'data/extracted'\n",
    "\n",
    "# Имена файлов с извлечёнными признаками для dense сети, линейных моделей и деревьев.\n",
    "extracted_train = 'features_labels_train' \n",
    "extracted_test = 'features_labels_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загружаем извлечённые признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, names, labels = load_obj(os.path.join(extracted_data, extracted_train))\n",
    "X_test, names_test = load_obj(os.path.join(extracted_data, extracted_test))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# кодируем таргет\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "ohe =  OneHotEncoder()\n",
    "y_ohe = ohe.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число классов: 8\n",
      "Число признаков: 193\n",
      "Число сэмплов: 11307\n"
     ]
    }
   ],
   "source": [
    "n_features = X.shape[1]\n",
    "n_classes = y.max()+1\n",
    "print('Число классов: {}\\nЧисло признаков: {}\\nЧисло сэмплов: {}'.format(n_classes, n_features, y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> для правильной валидации нужно учитывать, что некоторые сэмплы являются частями одной записи.\n",
    "Если части одной записи будут разрознено находится и в train и в val, то произойдёт утечка меток в валидацию.\n",
    "\n",
    "Поэтому сперва найдём уникальные записи и сгруппируем все фрагменты по принадлежности к отдельной записи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_le = LabelEncoder()\n",
    "unique_samples = pd.Series(names).apply(lambda x: x.split('time_stretch')[0].strip('.wav').strip('_'))\n",
    "groups = groups_le.fit_transform(unique_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8, random_state=7)\n",
    "\n",
    "idxs = np.arange(X.shape[0])\n",
    "tr_idxs, val_idxs = next(iter(gss.split(idxs, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сплитим...\n",
    "# имена файлов\n",
    "names_train, names_val = names[tr_idxs], names[val_idxs]\n",
    "# извлечённые признаки\n",
    "X_train, X_val = X[tr_idxs], X[val_idxs]\n",
    "# метки для sklearn\n",
    "y_train, y_val = y[tr_idxs], y[val_idxs]\n",
    "# метки для сеток\n",
    "y_ohe_train, y_ohe_val = y_ohe[tr_idxs], y_ohe[val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_train, groups_val = groups[tr_idxs], groups[val_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нелинейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### бустинг и лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grigory/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:: Train score: 1.0, val score: 0.9471132657558395\n",
      "CPU times: user 3min 53s, sys: 728 ms, total: 3min 54s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grigory/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(learning_rate=1e-2,\n",
    "#                        reg_alpha=1e-2,\n",
    "#                        reg_beta=1e-1,\n",
    "#                      valid_sets=[X_val, y_val],\n",
    "                       random_state=7,\n",
    "                     n_estimators=500)\n",
    "lgbm.fit(X_train, y_train);\n",
    "\n",
    "# Оцениваем качества на отложенной выборке\n",
    "print('Scoring...')\n",
    "tr_score = lgbm.score(X_train, y_train)\n",
    "val_score = lgbm.score(X_val, y_val)\n",
    "print('Acc:: Train score: {}, val score: {}'.format(tr_score, val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "лучшие параметры  для случайного леса были найдены с помощью RandomizedSearch. Поиск занимает около 40 мин.\n",
    "\n",
    "Параметры были записаны  в best_params, так что ячейки с перебором закомиченна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ~ 40 min\n",
    "# from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# estimator = RandomForestClassifier(random_state=7)\n",
    "# gs_params = dict(\n",
    "#     max_depth=[3,10, 15, 50, 100, 150],\n",
    "#     n_estimators=[5,10, 25, 50, 100, 150, 250],\n",
    "#     max_features= list(np.linspace(0.1,1,20)) + ['auto', 'sqrt'],\n",
    "#     bootstrap=[True, False]\n",
    "# )\n",
    "# gs = RandomizedSearchCV(estimator, gs_params, n_jobs=-1, random_state=7,\n",
    "#                         n_iter=100, verbose=2)#GridSearchCV(trees, gs_params)\n",
    "# gs.fit(X_train, y_train, groups=groups_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...\n",
      "Acc:: Train score: 1.0, val score: 0.9519612163948876\n"
     ]
    }
   ],
   "source": [
    "# trees = gs.best_estimator_\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "best_params = {'n_estimators': 100, \n",
    "               'max_features': 0.19473684210526315, \n",
    "               'max_depth': 100, 'bootstrap': False,\n",
    "               'random_state':12}\n",
    "trees = RandomForestClassifier(**best_params)\n",
    "trees.fit(X_train, y_train)\n",
    "# Оцениваем качества на отложенной выборке\n",
    "print('Scoring...')\n",
    "tr_score = trees.score(X_train, y_train)\n",
    "val_score = trees.score(X_val, y_val)\n",
    "print('Acc:: Train score: {}, val score: {}'.format(tr_score, val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 5 min\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# logreg = LogisticRegression(random_state=7)\n",
    "# gs_params = dict(\n",
    "#     C = np.logspace(-3,3,10),\n",
    "# )\n",
    "# gs = RandomizedSearchCV(logreg, gs_params, n_jobs=-1, random_state=7,\n",
    "#                         n_iter=10, verbose=1)#GridSearchCV(trees, gs_params)\n",
    "# gs.fit(X_train, y_train, groups=groups_train);\n",
    "\n",
    "# logreg = gs.best_estimator_\n",
    "# logreg.fit(X_train, y_train)\n",
    "# # Оцениваем качества на отложенной выборке\n",
    "# print('Scoring...')\n",
    "# tr_score = logreg.score(X_train, y_train)\n",
    "# val_score = logreg.score(X_val, y_val)\n",
    "# print('Acc:: Train score: {}, val score: {}'.format(tr_score, val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...\n",
      "Acc:: Train score: 0.9470015490152689, val score: 0.8973115910092552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "best_params = {'C': 1000.0}\n",
    "logreg = LogisticRegression(**best_params, random_state=7)\n",
    "logreg.fit(X_train, y_train)\n",
    "# Оцениваем качества на отложенной выборке\n",
    "print('Scoring...')\n",
    "tr_score = logreg.score(X_train, y_train)\n",
    "val_score = logreg.score(X_val, y_val)\n",
    "print('Acc:: Train score: {}, val score: {}'.format(tr_score, val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### онлайн-обучение нейонных сеток\n",
    "\n",
    "Обзор методов на нейронных сетях:\n",
    "\n",
    "- http://www.fim.uni-passau.de/fileadmin/files/lehrstuhl/schuller/Publications/Amiriparian17-SSC.pdf\n",
    "- https://github.com/libphy/which_animal\n",
    "- https://github.com/jaron/deep-listening\n",
    "- https://musicinformationretrieval.com/mfcc.html\n",
    "\n",
    "Тестировались:\n",
    "    - полносвязанные сети\n",
    "    - lstm\n",
    "    - свёрточные\n",
    "\n",
    "Самые лучшие результаты показали полносвязные сети.\n",
    "\n",
    "\n",
    "*Коротко о подборе оптимальной архитектуры для Dense сети*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=350 src=\"https://habrastorage.org/webt/sg/7t/tu/sg7ttuirleaml3_j7dwo2tn0iqs.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import (build_deep_dense, build_conv_seq, build_lstm_seq)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: val_loss improved from inf to 1.19687, saving model to models_weights/net_ff.h5\n",
      "Epoch 00008: val_loss improved from 1.19687 to 0.67102, saving model to models_weights/net_ff.h5\n",
      "Epoch 00012: val_loss improved from 0.67102 to 0.49071, saving model to models_weights/net_ff.h5\n",
      "Epoch 00016: val_loss improved from 0.49071 to 0.41781, saving model to models_weights/net_ff.h5\n",
      "Epoch 00020: val_loss improved from 0.41781 to 0.40695, saving model to models_weights/net_ff.h5\n",
      "Epoch 00024: val_loss improved from 0.40695 to 0.37263, saving model to models_weights/net_ff.h5\n",
      "Epoch 00028: val_loss improved from 0.37263 to 0.31864, saving model to models_weights/net_ff.h5\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 00048: val_loss improved from 0.31864 to 0.30467, saving model to models_weights/net_ff.h5\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 00060: val_loss improved from 0.30467 to 0.29988, saving model to models_weights/net_ff.h5\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 00068: val_loss improved from 0.29988 to 0.28557, saving model to models_weights/net_ff.h5\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 00076: val_loss improved from 0.28557 to 0.27811, saving model to models_weights/net_ff.h5\n",
      "Epoch 00080: val_loss improved from 0.27811 to 0.26570, saving model to models_weights/net_ff.h5\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 00100: val_loss did not improve\n",
      "\n",
      "Epoch 00100: reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 00108: val_loss did not improve\n",
      "\n",
      "Epoch 00110: reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 00112: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5004ab9e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_deep_dense(n_features, n_classes)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(1e-4), \n",
    "              metrics=['accuracy'])\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=25, monitor='val_loss'),\n",
    "    ModelCheckpoint('models_weights/net_ff.h5', monitor='val_loss', \n",
    "                    verbose=1, save_best_only=True, period=4),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n",
    "                      patience=10, verbose=1, mode='auto')\n",
    "]\n",
    "model.fit(X_train, y_ohe_train.todense(), \n",
    "          epochs=300, batch_size=250, \n",
    "          verbose=0,\n",
    "          validation_data=[X_val, y_ohe_val.todense()],\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Рекурретная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_lstm_seq(timesteps=20, data_dim=41, n_classes=n_classes)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=Adam(1e-4), \n",
    "#               metrics=['accuracy'])\n",
    "# callbacks = [\n",
    "#     EarlyStopping(patience=25, monitor='val_loss'),\n",
    "#     ModelCheckpoint('models_weights/net_lstm.h5', monitor='val_loss', \n",
    "#                     verbose=1, save_best_only=True, period=4)\n",
    "# ]\n",
    "\n",
    "# model.fit(X_train, y_ohe_train.todense(), \n",
    "#           epochs=300, batch_size=250, \n",
    "#           validation_data=[X_val, y_ohe_val.todense()],\n",
    "#          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "model = load_model('models_weights/net_ff.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9831821199380394, val score: 0.9519612163948876\n"
     ]
    }
   ],
   "source": [
    "from utils import inverse_ohe\n",
    "preds = model.predict(X_train)\n",
    "preds_labels = inverse_ohe(preds, ohe)\n",
    "tr_score = accuracy_score(y_train, preds_labels)\n",
    "preds = model.predict(X_val)\n",
    "preds_labels = inverse_ohe(preds, ohe)\n",
    "te_score = accuracy_score(y_val, preds_labels)\n",
    "\n",
    "print('Train score: {}, val score: {}'.format(tr_score, te_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы по моделям:\n",
    "\n",
    "> Наилучшими моделями(без яростного тюнинга параметров) оказались полносвязная сеть, бустинг и случайный лес. В целом, можно сказать, что они дают  сопоставимые результаты, но если посмотреть на:\n",
    "    - распределние плотности вероятности для максимальных классов по всей выборке\n",
    "    - разницу точности предсказаний на трейне и валидации \n",
    "...можно легко заметить, что модели переобучились. Чтобы бороться с переобучением сделаем простой блендинг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блендинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:: Train score: 0.9962381057756141, val score: 0.9638607315998237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def blend(models, X, weights=None, proba=False):\n",
    "    preds = []\n",
    "    weights = weights or [1/len(models), ] * len(models)\n",
    "    for model, weight in zip(models, weights):\n",
    "        preds.append(model.predict_proba(X)* weight)\n",
    "    if proba:\n",
    "        return np.stack(preds).sum(axis=0)\n",
    "    else:\n",
    "        return np.stack(preds).sum(axis=0).argmax(axis=1)\n",
    "\n",
    "models = [trees, model, logreg, lgbm]\n",
    "weights = [0.30, 0.40, 0.25, 0.05]\n",
    "tr_score = accuracy_score(y_train, blend(models, X_train, weights))\n",
    "val_score = accuracy_score(y_val, blend(models, X_val, weights))\n",
    "print('Acc:: Train score: {}, val score: {}'.format(tr_score, val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предикт теста и подготовка сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict(X_test)\n",
    "# preds = trees.predict(X_test)\n",
    "\n",
    "preds = blend(models, X_test, weights=weights, proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grigory/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "def prepare_submit(names, preds, ohe=ohe, le=le):\n",
    "    pred_labels = inverse_ohe(preds, ohe)\n",
    "    pred_names = le.inverse_transform(pred_labels)\n",
    "    df = pd.DataFrame(list(zip(names, preds.max(axis=1), pred_names)), columns=['file','prob','label'])\n",
    "    return df\n",
    "                      \n",
    "submit_df = prepare_submit(names_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_label\n",
       "background    0.641667\n",
       "bags          0.854057\n",
       "door          0.683569\n",
       "keyboard      0.816699\n",
       "knocking      0.809235\n",
       "ring          0.899164\n",
       "speech        0.778281\n",
       "tool          0.781356\n",
       "unknown       0.598940\n",
       "Name: prob, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df['file_label'] = submit_df['file'].apply(lambda x: x.split('_')[0])\n",
    "submit_df.groupby('file_label')['prob'].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv('result.txt', sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
